{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim: Create a reinforcement learning model to investigate the evolution of vector-borne plant pathogens\n",
    "\n",
    "Pathogen aims: maximise tranmission across both hosts and vectors. Requires optimal trade-off between infectivity and virulence\n",
    "Vector aims: maximise survivial. Move, seek resources, avoid dying. Vectors do not know they carry a pathogen, but their behaviour indirectly affects pathogen transmission. Movement patterns influenced by: host availability, environmental conditions, avoiding areas where hosts are dying / scarce.\n",
    "\n",
    "Could also have different vector species, and generalist / specialist pathogens?\n",
    "\n",
    "Hosts: become infected and die. \n",
    "\n",
    "## Interactions between pathogen and vector.\n",
    "\n",
    "No direct interaction. Pathogen and vector do not communicate or know about each other. Pathogen can only control its internal parameters (infectivity and virulence), and the vector acts based on its own needs (survial and movement).\n",
    "Indirect feedback: The pathogen's infectivity and virulence affect vector behaviours by influencing host availability and health.\n",
    "\n",
    "## Multi-Agent Reinforcement Learning (MARL) Setup\n",
    "\n",
    "Pathogen as an RL agent.\n",
    "State\n",
    "- Health of current host\n",
    "- number of vectors nearby\n",
    "- host density\n",
    "Actions: control infectivity and transmission\n",
    "Reward:\n",
    "- transmission success: rewarded when new host or vector becomes infected\n",
    "- host death penalty: when host dies before optimal transmission occurs\n",
    "- long-term survival: pathogen should aim for strategies that balance infectivity and virulence to maximise the number of future hosts / vectors available for infection\n",
    "\n",
    "Vector as an RL Agent:\n",
    "State\n",
    "- availbility of nearby hosts\n",
    "- Environmental factors: temperature, humidity, presence of other vectors\n",
    "- health of potential hosts\n",
    "\n",
    "Actions:\n",
    "- movement: moving between hosts, or seeking new areas with more hosts\n",
    "- feeding behaviour: whether to feed on availble host or move on\n",
    "- resting / reproducing\n",
    "Reward:\n",
    "- survival\n",
    "- reproductive success: feeding success increases survival and the ability to reproduce\n",
    "- minimising time in hostile environments\n",
    "\n",
    "## Model implementation\n",
    "1. Pathogen learning algorithm. Q-learning or policy gradient methods (like Proximal Policy Optimisation (PPO)). Given that the pathogen only adjusts infectivity and virulence, the state-action space may be relatively manageable.\n",
    "2. Vector learning alogrithm. MARL approaches, sich as Deep Q-Networks (DQN) to learn movement and feeding strategies. Multi-agent actor-critic (A2C) methods to allow for continuous control (if vector movement is continuous rather than grid-based).\n",
    "3. Simulating evolutionarly dynamics. Introduce mutation and selection mechanisms for the pathogens to simulation evolution over time. Pathogens with higher fitness reproduce, while those that fail to spread die out. This could lead to evolutionarily stable strategies (ESS) where the pathogen evolves an optimal balance between infectivity and virulence in response to the vector population's behaviour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Step 1. Create the vectors and their environment.\n",
    "\n",
    "# The field is initialised as a 100 x 100 grid. 20% of the grid cells contain hosts. Each host is representing by a number between 0 and 1, which represents the quality of the host.\n",
    "\n",
    "def create_field(xdim, ydim):\n",
    "    field = np.random.rand(xdim,ydim)\n",
    "    ## Set 80% of the field to 0\n",
    "    # Flatten the field to a 1D array\n",
    "    flat_field = field.flatten()\n",
    "\n",
    "    # Calculate the number of elements to set to 0\n",
    "    num_elements_to_zero = int(0.8 * flat_field.size)\n",
    "\n",
    "    # Randomly select indices to set to 0\n",
    "    indices_to_zero = np.random.choice(flat_field.size, num_elements_to_zero, replace=False)\n",
    "\n",
    "    # Set the selected indices to 0\n",
    "    flat_field[indices_to_zero] = 0\n",
    "\n",
    "    # Reshape the array back to its original 2D shape\n",
    "    field = flat_field.reshape(100, 100)\n",
    "\n",
    "    # Print the resulting field to verify\n",
    "    return field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Create the field (2D matrix) representing host quality (between 0 and 1)\n",
    "def create_field(size=100):\n",
    "    return np.random.rand(size, size)  # Returns a size x size matrix with random values between 0 and 1\n",
    "\n",
    "class Leafhopper:\n",
    "    def __init__(self, x, y, field, alpha=0.1, gamma=0.9, epsilon=0.1, inherited_q_table=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.field = field\n",
    "        self.energy = 0.5  # Start with some initial energy\n",
    "        self.age = 0\n",
    "        self.reproduction = 0\n",
    "        self.mutation_rate = 0.1  # Mutation rate for offspring's behavior\n",
    "        self.energy_threshold = 0.6  # Adjust reproduction threshold\n",
    "        self.age_threshold = 20  # Increase age threshold\n",
    "        self.energy_decay = 0.01  # Lose energy each step\n",
    "        \n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        # Initialize Q-table or inherit it from a parent\n",
    "        if inherited_q_table is None:\n",
    "            self.q_table = {}  # Initialize new Q-table\n",
    "        else:\n",
    "            self.q_table = self.mutate_q_table(inherited_q_table)  # Mutate the parent's Q-table\n",
    "        \n",
    "        self.actions = [\"move\", \"eat\", \"rest\"]  # Possible actions\n",
    "    \n",
    "    def mutate_q_table(self, q_table):\n",
    "        mutated_q_table = {}\n",
    "        for state, q_values in q_table.items():\n",
    "            if isinstance(q_values, np.ndarray):\n",
    "                mutated_q_values = q_values + np.random.normal(0, self.mutation_rate, size=q_values.shape)\n",
    "            else:\n",
    "                mutated_q_values = np.random.normal(0, self.mutation_rate, size=len(self.actions))\n",
    "            mutated_q_table[state] = mutated_q_values\n",
    "        return mutated_q_table\n",
    "    \n",
    "    def get_state(self):\n",
    "        # State is a tuple of the current position and energy\n",
    "        state = (self.x, self.y, self.energy)\n",
    "        return state\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < self.epsilon:  # Exploration\n",
    "            return np.random.choice(self.actions)\n",
    "        else:  # Exploitation\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = np.zeros(len(self.actions))  # Initialize Q-values\n",
    "            return self.actions[np.argmax(self.q_table[state])]\n",
    "\n",
    "    def move(self):\n",
    "        # Move to the neighbouring cell with the highest resources or a random neighbor\n",
    "        neighbours = []\n",
    "        best_quality = -1\n",
    "        best_pos = None\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                if i == 0 and j == 0:\n",
    "                    continue\n",
    "                new_x = self.x + i\n",
    "                new_y = self.y + j\n",
    "                if 0 <= new_x < self.field.shape[0] and 0 <= new_y < self.field.shape[1]:\n",
    "                    neighbours.append((new_x, new_y))\n",
    "                    if self.field[new_x, new_y] > best_quality:\n",
    "                        best_quality = self.field[new_x, new_y]\n",
    "                        best_pos = (new_x, new_y)\n",
    "\n",
    "        # Randomly choose to move to a random neighbor or the best neighboring cell\n",
    "        if np.random.rand() < 0.1:  # 10% chance to move randomly\n",
    "            if neighbours:\n",
    "                self.x, self.y = random.choice(neighbours)\n",
    "        elif best_pos:\n",
    "            self.x, self.y = best_pos\n",
    "\n",
    "    \n",
    "    def eat(self):\n",
    "        # Leafhopper eats part of the resources in the cell, leaving some for regeneration\n",
    "        self.energy += self.field[self.x, self.y] * 0.5  # Consume 50% of available resources\n",
    "        self.field[self.x, self.y] *= 0.5  # Resources get reduced by 50%\n",
    "\n",
    "    def rest(self):\n",
    "        pass  # Rest: Do nothing for one time step\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        if action == \"move\":\n",
    "            self.move()\n",
    "        elif action == \"eat\":\n",
    "            self.eat()\n",
    "        elif action == \"rest\":\n",
    "            self.rest()\n",
    "    \n",
    "    def get_reward(self):\n",
    "        # Reward for energy accumulation and penalize for aging without action\n",
    "        if self.energy >= self.energy_threshold:\n",
    "            return 10  # Positive reward for survival and reproduction potential\n",
    "        elif self.energy == 0:\n",
    "            return -10  # Penalty for starvation\n",
    "        else:\n",
    "            return -1  # Small penalty to encourage activity\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        # Convert action to an index\n",
    "        action_idx = self.actions.index(action)\n",
    "        \n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "        \n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(len(self.actions))\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        self.q_table[state][action_idx] += self.alpha * (\n",
    "            reward + self.gamma * self.q_table[next_state][best_next_action] - self.q_table[state][action_idx]\n",
    "        )\n",
    "\n",
    "    def step(self):\n",
    "        # Perform one step in the environment\n",
    "        state = self.get_state()\n",
    "        action = self.choose_action(state)\n",
    "        self.take_action(action)\n",
    "        reward = self.get_reward()\n",
    "        next_state = self.get_state()\n",
    "        self.update_q_table(state, action, reward, next_state)\n",
    "        self.energy -= self.energy_decay  # Lose energy per step\n",
    "        self.age += 1\n",
    "        return reward\n",
    "\n",
    "    def die(self):\n",
    "        # Leafhopper dies if it exceeds age threshold or runs out of energy\n",
    "        return self.age >= self.age_threshold or self.energy <= 0\n",
    "\n",
    "    def reproduce(self):\n",
    "        if self.energy >= self.energy_threshold:\n",
    "            self.energy /= 2  # Share energy with offspring\n",
    "            return Leafhopper(self.x, self.y, self.field, inherited_q_table=self.q_table)\n",
    "\n",
    "# Function to regenerate the field\n",
    "def regenerate_field(field, growth_rate=0.01):\n",
    "    field += growth_rate\n",
    "    np.clip(field, 0, 1, out=field)  # Ensure values stay within [0, 1]\n",
    "\n",
    "def run_simulation(num_epochs=100, num_hoppers=50, field_size=100):\n",
    "    field = create_field(size=field_size)\n",
    "    leafhoppers = [Leafhopper(random.randint(0, field_size-1), random.randint(0, field_size-1), field) for _ in range(num_hoppers)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    img = ax.imshow(field, cmap='YlGn_r', origin='lower')\n",
    "    scatter = ax.scatter([hopper.x for hopper in leafhoppers], \n",
    "                         [hopper.y for hopper in leafhoppers], c='red', label=\"Leafhoppers\")\n",
    "\n",
    "    def update(epoch):\n",
    "        regenerate_field(field)\n",
    "        new_leafhoppers = []\n",
    "        for hopper in leafhoppers[:]:\n",
    "            if not hopper.die():\n",
    "                hopper.step()\n",
    "                offspring = hopper.reproduce()\n",
    "                if offspring:\n",
    "                    new_leafhoppers.append(offspring)\n",
    "            else:\n",
    "                leafhoppers.remove(hopper)\n",
    "\n",
    "        leafhoppers.extend(new_leafhoppers)\n",
    "\n",
    "        img.set_array(field)\n",
    "        if leafhoppers:\n",
    "            x_positions = [hopper.x for hopper in leafhoppers]\n",
    "            y_positions = [hopper.y for hopper in leafhoppers]\n",
    "            scatter.set_offsets(np.column_stack((x_positions, y_positions)))\n",
    "        ax.set_title(f\"Epoch {epoch}: {len(leafhoppers)} leafhoppers\")\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=num_epochs, repeat=False)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simulation(num_epochs=2000, num_hoppers=500, field_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
